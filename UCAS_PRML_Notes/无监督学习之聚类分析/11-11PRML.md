## 有监督和无监督

### 有监督的场景设定

>对于给定的<x^i, y^i>, 学习y^=f(x)
>
>分类，y是类别
>
>回归，y是连续数
>
>排序，y是序值（信息检索，搜索结果
>
>> 最早的时候是一个分类问题（相关和不相关
>>
>> 但应该是一个序数

### 无监督学习场景

> 对于给定的<x^i>, 学习y^=f(x)
>
> 密度估计：y是密度
>
> 聚类：y是类簇（聚类
>
> 数据规约/可视化：y是x的低纬度表示（AutoEncoder

### 无监督学习的必要性

**拿不到y**。业界可以砸钱标注，比如ImageNet。

降低维，减少高维数据中的噪声。（这个过滤噪声的过程一般不能是时域上的，更多的是变换域上的。利用DL等方法可以解决

可解释的数据分析

**经常被作为监督学习的预处理步骤**。这是当前监督学习的有种流行的范式。



## 聚类分析

发现数据中的聚团效应——簇。簇内部紧密，簇和簇之间离散。“簇”这个东西的定义是非常广泛的，在不同的场景下可以定义不同的簇，发展出不同的方法。

#### 不同的聚类方式

簇的定义不够严格，对于聚类的结果可以考虑层次的描述方法：（是的，可以，下面就讲了不同的聚类方法描述

![image-20191111134701712](/Users/shaozezhi/Library/Application Support/typora-user-images/image-20191111134701712.png)

分类方式一：

1. 基于划分的聚类（无嵌套）
2. 层次聚类（嵌套）

其他分类方式：

#### 不同的簇的类型

- 基于中心的簇

  中心常用质心表示，即簇内所有点的平均。或用中心点表示，即最有代表性的点。

- 基于密度的簇

  簇是高密度区域形成的

- 基于连续性的簇

可以看到，簇的样本的类型，完全依赖于对“空间聚集“这一概念的假设。不同的假设方式，就有不同的聚类方法。

- 基于概念的簇

#### 聚类的应用

Image Segmentation

层次聚类——人类种族分析

复杂网络分析——社交网络聚类分析，用来发现一些社区、群体。（这一部分的应用非常广泛

其他应用

- 用户画像
- 

#### 聚类分析的三要素

- 定义远近：使用相似性/距离函数
- 评价聚类簇的质量
- 如何获得聚类的簇



## 定义距离函数

非常严重的依赖应用：

- 文档聚类
- 图像分割
- 用户画像

为了量化样本，我们总是要找一个度量函数：

- 距离、相似性、不相似性、临近函数的选择于应用
- 考虑数据的特性
  - 类别
  - 序值
  - 数值
- 直接从数据中学习相似性/距离函数（这个应该挺难的，更复杂一些

#### 距离函数

1. 距离函数的要求：

2. Minkowski聚类
   - 欧式距离
   - 马哈顿距离
3. 这类距离对于特征的旋转和平移变换不敏感，但是对于数值尺度敏感
4. 对尺度敏感，那么假如特征值尺度不一样，就必须**标准化数据**，否则特征值尺度不一致将会造成非常大的影响。scale特大的会占据主导。



#### 标准化数据

- Z-score Normalization：均值为0方差为1。

- Min-max
- Decimal scaling

标准化并不一定起效！

#### 其他的相似/不相似函数

- A交B / A并B——Jaccard系数
- Cos similarity
- 。。。



## 评价指标

聚类的性能评价：有效性指标(validity index)

评价指标：

- 有参考模型（外部指标）但是大多数情况下都没有参考模型，没有ground truth。但是作为研究来说，可以先设定好groundtruth，用来验证无监督学习算法的性能。
- 无参考模型

### 参考模型

### 无参考模型

只能回归簇的定义：**簇内越相似越好、簇间相似度越低，聚类质量越好**。

#### 簇内相似度

- 平均聚类
- 最远距离：半径有多大

#### 簇间相似度

- 最小距离（最近的两个点）
- 中心点之间的距离

####  内部指标

- DBI，和聚类质量反比
- DI，正比于聚类质量

## 聚类算法

### K均值聚类

最基本、最简单、最广泛使用的算法。

- 算法过程略

####为什么可以说他是个好算法？弄明白它到底在干什么——假设是什么、优化目标是什么。

>K-means是一个划分型的聚类。
>
>1. 如何表示簇
>
>   每个簇用之心表示
>
>2. 如何划分节点
>
>   使用欧式距离进行度量
>
>   咩个节点划分到最近的那个质心的簇中
>
>   r<sub>ik</sub>为丛书度。指示
>
>3. 优化目标：
>
>   实际上是在优化一个损失函数——平房误差和函数SSE
>
>4. K-means又一个突出的问题，是由于他的计算方式隐含了一个假设：
>
>   簇的球形的。
>
>5. 基于中心的簇的假设，很多都是隐含了簇是球形的假设。
>
>6. 如何优化
>
>   是一个鸡和蛋的问题：使用迭代和随机初始化的方式。

#### 这个过程能够保证停下来吗，过程会收敛吗？

#### 到底该聚成几个类——如何确定K

- K是算法的一个超参

#### 如何初始化K-means

都是启发式的方法：

- 随机
- 选择第个数据点作为中心点个中心时，选择与距离之前选出的中心点距离最远的
- 预处理
- 后处理

#### K-means局限性

- 尺度不同
- 密度不同
- 非球形

K-means可能得不到理想的效果

#### 克服局限性





### 高斯混合模型和EM算法

TODO:总结高斯混合模型及其EM算法

>https://www.bilibili.com/video/av29430384?p=13
>
>https://blog.csdn.net/jinping_shi/article/details/59613054



#### 基本定义

#### 混合高斯模型的学习过程困难

- 参数学习：极大似然估计，最大化log likelihood
- 

#### EM算法

#### EM算法的通用视角

log函数很难解，因为有隐变量还有求和。

#### 泛化的EM视角

### 小结K-means vs 高斯混合模型

### 层次聚类

- 不需要指定簇的数量
- 聚类结果对应着分类体系，这些分类体系在现实生活中是有实际意义的。比如生物的界门纲目科属中。

用户可以在某个地方截断，取当前的分类结果

簇之间的相似性在层次聚类中非常重要，这关乎要不要合并

#### 如何定义簇间的相似性

- 最小距离——可以形成非球形的、非图的簇，但是有链式效应
- 最大距离
- 平均
- 中心点
- Ward's方法

#### 层次聚类的限制

凝聚的过程是一个贪心过程——再也不会被拆

写不出一个全局优化目标

不同的方法存在一些问题

### DBSCAN

簇的定义：高密度的





## 一些其他的思考

在机器学习中，数据大概可以分成四大类：图像 (Image)，序列(Sequence)，图(Graph) 和表格(Tabular) 数据。其中，前3类数据有比较明显的模式，比如图像和图的空间局部性，序列的上下文关系和时序依赖等。而表格数据常见于各种工业界的任务，如广告点击率预测，推荐系统等。在表格数据中，每个特征表示一个属性，如性别，价格等等，特征之间一般没有明显且通用的模式。



神经网络适合的是前三类数据，也就是有明显模式的数据。因为我们可以根据数据的模式，设计对应的网络结构，从而高效地自动抽取“高级”的特征表达。如常见的 CNN (卷积神经网络) 就是为图像而设计的，RNN (循环神经网络) 为序列数据而设计的。而表格数据，因没有明显的模式，非要用神经网络的话，就只能用低效的全连接网络，一般效果都不太好。在实践中，对于表格数据，除了专门对特定任务设计的网络结构如DeepFM等，更多时候还是用传统机器学习模型。尤其是 GBDT (梯度提升树)，因其自动的特征选择能力及动态的模型复杂度，算得上是一个万金油模型，在各种类型的表格数据上都表现很好。但对于表格数据而言，其实特征工程才是更关键的。在给定数据的情况下，模型决定了下限，特征决定了上限。特征工程类似于神经网络的结构设计，目的是把先验知识融入数据，并且让模型更好地理解数据，让模型可以学得更好。



另外，神经网络实质上不算是一个模型，而是一类可以自由“搭积木”的模型。结构不同的神经网络可以认为是不同的模型了。

总结下，no free lunch，没有一个万能的模型，可以直接用于各种数据。有多少人工就有多少智能：用神经网络的话，你需要结构设计；而用传统模型的话，你需要特征工程。

