- 维度灾难
  - max distance随着维度增加，逐渐接近min distance
  - 就算是计算夹角也是一样的情况

- 维数灾难
  - 使用的算法需要用到距离和相似性计算时
  - 共线性引起的问题，希望得到一个简单模型
  -  计算和存储复杂度成为难题
- 数据降维或者嵌入，他的本质想法是，原始数据经常是特征冗余的。真正的，能够刻画问题的维度，没有那么高。



- 降维方法：
  - 特征选择/维度选择
  - 特征提取/维度抽取



## 维度选择

### 随机映射/随机采样

随机抽取一些维度作为新的样本。这个定理保证根据这个随机采样，新的样本的内机的关系和原来的关系，来证明有一定的损失。

缺点：

- 没有精度保证，特别是在稀疏的数据上（碰巧采样到了那些0）

- 重尾分布，特征分布的那些值，在尾巴上很多。

### 手工移除特征

- 冗余的：经济学里面肥西多重共线性...



- 不相关的：文本里面常用的停用词表：The a ... 出现很多次，没什么信息量。一个词假如出现在了很多很多的文档里，那么这个词的信息量就不大。



- 质量差的特征：残缺过多

### 监督学习的方法

专门做一个分类器，在一个小的数据集上，算：

- 互信息
- 相关性

从没有特征开始，根据特征的得分，一个一个加特征。（消融的思想）

还有一种是一个一个减



## 维度抽取

研究最活跃，成果最多

### 预备矩阵知识

1. 矩阵的秩：最大的线性无关行（或者列）的个数。

- **Row Rank = Column Rank**
- rank(A)<=min(m,n)
- **rank(AB)<=min(rank(A), rank(B))**

2. 矩阵的迹：对角线上的元素和。

3. 矩阵的乘法=线性变换
   - m X n的一个实数矩阵A对于从Rn->Rm的变换。
4. 矩阵分解，用低秩的矩阵逼近原来的矩阵
5. 特征分解
6. 奇艺值分解
   1. 不是方针的时候分解
      1. 左奇艺响亮
      2. 右奇艺向量
   2. 它和特征分解有紧密的关联
7. 特征值或奇艺值的意义
   1. 统计角度：特征值-刻画方差
   2. 物理角度：特征值-刻画系统能量
   3. 奇艺值也一样：每一个左/右都代表一个方向

### MDS：多维缩放

这类问题比较特殊：**只能知道给定空间中任意两个点的距离**，**点的精确坐标和维度是未知的**。

1. 形式化：d′空间的欧式距离等于原始空间的欧式距离
   1. 先计算样本点之间的内积矩阵。

### 线性降维

1. PCA
   1. 目标函数1:最小化重建误差
   2. 最大化方差
2. PCA面临的挑战：原始维度非常大，会导致协方差矩阵更大
3. SVD可以帮忙找到特征向量。
4. 优缺点



### 非线性方法

Kernel PCA



























