- 两大体系
  - 归纳学习：开放的世界
  - 直推学习：封闭的世界



## 半监督学习背后的假设

#### 平滑假设

如果高密度空间中两个点𝑥(1), 𝑥 (2)距离较近,那么对应的输出𝑦 (1) , 𝑦 (2)也应该接近。

#### 聚类假设

#### 流形假设





## 半监督学习算法

### 自学习算法

自己交自己——把模型输出的高置信度，非常有信心的数据作为新的Groundtruth。

最关键的一点，是到底用那些输出作为GT。

- 加入最置信的数据（不同的算法的不同指标中最好的那一些，要极限好）
  - 这个数据可能会很小
- 都加进去，但是带权重的加进去
  - 数据量多，但是可能不够好



#### 评价

- 最简单的方法
- 这是一个框架，是一个学习策略，可以到所有已有的监督模型中。
- 优势
- 劣势
  - 早期的错误会强化
    - 启发式的缓解方法，置信度分数低于某个阈值就去掉
  - 收敛性方面会没有保障
    - 当然也有特例，就是当自我训练等驾驭EM算法
    - 有部分存在封闭解的特殊情形

### 多视角学习

协同训练：数据对象可能有不同的视角——图像、文本等。

利用不同的视角互相促进他的学习。

#### 暗含的假设

1. 这些视角应该是从分的：单独用这个视角就能干成一些事，即这个视角本身就挺好的。

2. 这两个条件在给定类别后是条件独立的。

#### 优缺点

- 优点
  - 也是wrapper的方法
  - 相比于self-training对于错误不那么敏感（因为前面的假设）
- 缺点
  - 自然的特征分裂可能不存在
  - 全部特征的模型可能效果更好

#### 协同训练的变体

- Co-EM
- 假的特征集分裂

#### 为什么多视角能学好

- 协同学习
- 学习的过程是搜索最好的分类器。而多个分类器的预测性一致的要求，其实就是减少了搜索的空间。
- 获得了一些理论结果的支持
- 基于多视角的半监督学习是半监督学习和集成学习的自然过渡
  - 集成学习：对一个分类任务，学多个分类器，学出来的这一组分类器组合投票。
  - 半监督学习的观点，事物是分为多个view的，多个view得到多个分类器，综合得到结果。
  - （集成学习对view没有要求，可以是一个view。）

### 生成式模型

高斯混合模型——无监督，用来聚类（分类）

#### 和有监督生成模型的不同

#### 假设

所有数据（无论标注与否）都是由同一个潜在的模型“生成”的。

#### 一些例子

